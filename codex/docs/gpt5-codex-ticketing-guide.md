
# Writing Excellent Tickets for **GPTâ€‘5 Codex** (Evidenceâ€‘Based, Bestâ€‘Practices Guide)
*Version: 2025â€‘09â€‘22*

This guide distills what works in practice when you want GPTâ€‘5 Codex (CLI/IDE/Cloud) or the GPTâ€‘5 API to **design, implement, and land PRâ€‘quality code** from a ticket/spec. It integrates OpenAIâ€™s official guidance on prompting, tool use, structured outputs, reproducibility, and agentic workflows, plus fieldâ€‘tested heuristics for running AI in real engineering orgs.

---

## 0) What â€œexcellentâ€ tickets do
An excellent ticket for GPTâ€‘5 Codex is **specific enough to be unambiguous** but **narrow enough to be tractable in one PR**. It does four things:

1. **Pinpoints scope** â€” exact files, symbols, or endpoints; what *not* to touch; constraints and nonâ€‘goals.  
2. **Defines success** â€” runnable verification steps, acceptance tests, and exit criteria.  
3. **Supplies context** â€” repo coordinates, stack/version/constraints, style conventions, and links to relevant code.  
4. **Specifies output form** â€” *diff vs. wholeâ€‘file*, PR title/body template, and what to emit besides code (e.g., decision log).

OpenAIâ€™s Codex docs emphasize **clear code pointers, verification steps, and splitting work**; treat your ticket as the control surface for those behaviors.

---

## 1) Ticket blueprint (copy/paste)

Use this as your default issue template. Keep it short but complete. Replace ðŸ”· with your values.

```yaml
title: "[ðŸ”·project] ðŸ”·concise, outcomeâ€‘oriented title"
type: feature|bugfix|refactor|docs|infra
priority: P0|P1|P2
branch: "ðŸ”·feature/slug"
repo:
  url: ðŸ”·
  root: ðŸ”·
  paths_in_scope:
    - ðŸ”·/path/one
    - ðŸ”·/path/two
  paths_out_of_scope:
    - ðŸ”·/do-not-touch
context:
  stack: {lang: ðŸ”·, framework: ðŸ”·, runtime: ðŸ”·, package_manager: ðŸ”·}
  versions: {ðŸ”·: ðŸ”·}
  style_guides: [ðŸ”·link-to-lint/format rules]
  related_issues: [ðŸ”·]
problem_statement: >
  ðŸ”· crisp description of the user-facing change or bug to fix, with business value.
non_goals:
  - ðŸ”·
constraints:
  - Do not change public API of ðŸ”·
  - Keep allocations under ðŸ”·MB; P95 latency under ðŸ”·ms
  - Follow ðŸ”·security policy; no new network calls
artifacts_expected:
  output_mode: diff|whole_file
  pr_title_template: "[ðŸ”·component] ðŸ”·"
  pr_body_sections: ["Context", "Implementation", "Tests", "Risks", "Followâ€‘ups"]
  decision_log: true   # short bullet log of assumptions/tradeâ€‘offs
acceptance_criteria:
  - [ ] ðŸ”· Given/When/Then #1
  - [ ] ðŸ”· Given/When/Then #2
verification:
  repro_steps: |
    ðŸ”· commands / inputs to reproduce
  checks:
    - cmd: "ðŸ”·lint / ðŸ”·typecheck / ðŸ”·tests"
    - cmd: "ðŸ”·example e2e script with exit code 0 on success"
workflow_expectations:
  - plan-first
  - implement
  - self-check
  - emit_patch
  - run_verification
  - summarize

```

**Why this shape works**  
- **Clear pointers** let Codex and GPTâ€‘5 jump to the right code quickly.  
- **Verification commands** enable agents to prove theyâ€™re done.  
- **Output contract** + **Structured Outputs** let you parse and apply patches safely.  
These mirror the **Codex prompting guide** and **GPTâ€‘5 agentic prompting** recommendations. 

---

## 2) Prompt patterns that boost coding quality

You can embed these patterns in the **ticket** (for Codex).

### 2.1 Planâ€‘Implementâ€‘Verify (PIV)
**Instruction snippet**
```
Before editing code:
1) Write a minimal plan (bullet list). 
2) Make the smallest viable change (diff or whole-file). 
3) Run the verification commands from the ticket. 
4) If any check fails, fix and re-run.
5) Emit final patch, then a short decision log.
```
Rationale: aligns with GPTâ€‘5â€™s agentic preambles and toolâ€‘aware behavior.

### 2.2 Diffâ€‘first editing
Ask for **unified diff** or **search/replace blocks** to minimize token use and review overhead. GPTâ€‘4.1 and later improved adherence to diff formats; GPTâ€‘5 inherits and surpasses this reliability. 

**Instruction snippet**
```
Emit patches as JSON per `code_patch_v1` schema. For large rewrites, include full file content in `content` with format `whole_file`.
```

### 2.3 Rationale without chainâ€‘ofâ€‘thought
Prefer a **short explanation** (bullet rationale/decision log) over freeâ€‘form chainâ€‘ofâ€‘thought. GPTâ€‘5â€™s cookbook notes such brief summaries can improve instructionâ€‘following without exposing verbose reasoning.


### 2.4 Split large epics
Turn epics into **independent, verifiable subâ€‘tickets** (API contract, component, or service boundaries). Codex docs explicitly recommend splitting to improve throughput and reviewability.

---

## 3) Heuristics by task type

### 3.1 Bug fix
- **Ticket**: include failing test/logs, exact repro, suspected module, and nonâ€‘goals (donâ€™t refactor unrelated code).  
- **Prompt**: PIV pattern + diffâ€‘first + acceptance checks.

### 3.2 Feature slice (small)
- **Ticket**: new interface contract (types/endpoint), constraints (latency/allocations), and copy exact UX acceptance cases.  

### 3.3 Multiâ€‘file refactor
- **Ticket**: name the symbols to rename/move; list directories in/out of scope; provide `codemod` rules if you have them; insist on green types/tests.  

### 3.4 Frontend polish
- **Ticket**: include screenshots/UX rules; define visual and a11y acceptance criteria; perf budgets.  

### 3.5 Test authoring
- **Ticket**: name frameworks, fixtures, and the **exact** commands to run; require coverage deltas in the PR body.  

### 3.6 Docs / READMEs / ADRs
- **Ticket**: target audience and scope; must link to the code it documents.  

---

## 4) Codexâ€‘specific ticket addenda

For Codex (CLI/IDE/Cloud), add these **operational lines** so the agent behaves predictably:

```yaml
codex:
  approvals: on-request            # don't run untrusted commands without asking
  sandbox: workspace-write         # default sandbox; network disabled
  web_search: false                # enable only with allowlist
  progress: brief                  # tool preambles / progress updates
  review:
    ask_for_diff_format: unified   # patch preference
    pr_template: "ðŸ”· link to template"
  agents_md: true                  # ensure AGENTS.md exists and is up-to-date
```

- The Codex prompting guide highlights **pointers**, **verification**, **customization**, and **splitting tasks**; reflect those explicitly.
- Codex security docs recommend **patchâ€‘based workflows** and show how to configure sandbox & approvals.

---

## 5) Antiâ€‘patterns (what to avoid)
- **Vague scopes**: â€œImprove performanceâ€ without a target or benchmark.  
- **Openâ€‘ended rewrites**: allow the model to reâ€‘architecture without an ADR.  
- **No verification**: tickets without runnable checks tend to drift.  
- **Diff + wholeâ€‘file mixed randomly**: pick a primary mode and explain when to switch.  
- **Long prompts with moving parts at the top**: breaks prompt caching benefits. 

---

## 6) Example: â€œAdd optimistic updates to Todo listâ€ (React + TS)

```yaml
title: "[web] Todo optimistic updates for create/delete"
type: feature
branch: "feature/todo-optimistic-updates"
repo:
  root: apps/web
  paths_in_scope: ["src/features/todos/*", "src/lib/api.ts"]
  paths_out_of_scope: ["src/features/profile/*"]
context:
  stack: {lang: TypeScript, framework: React 18, runtime: Node 20, package_manager: pnpm}
  style_guides: ["apps/web/.eslintrc.js","apps/web/.prettierrc"]
problem_statement: >
  Improve perceived performance by applying optimistic UI updates for creating and deleting todos.
constraints:
  - Keep bundle increase < 2KB gz.
  - No new runtime deps.
acceptance_criteria:
  - [ ] Creating a todo updates the list immediately; if server fails, UI rolls back with a toast.
  - [ ] Deleting a todo removes it immediately; failure rolls back and toasts.
  - [ ] No console errors; all tests pass.
verification:
  repro_steps: |
    pnpm --filter web dev
    # test by creating/deleting todos
  checks:
    - cmd: "pnpm --filter web lint && pnpm --filter web typecheck && pnpm --filter web test -w"
codex_guidance:
  pointers: ["src/features/todos/TodoList.tsx", "src/lib/api.ts"]
  split_large_tasks: true
  preambles: brief
```

Expected agent flow (PIV): plan â†’ patch (diff) â†’ run checks â†’ summarize.

---

## 7) Quick checklist (paste into every ticket)

- [ ] **Scope**: files/symbols in/out; nonâ€‘goals included  
- [ ] **Pointers**: paths, stack traces, or identifiers included (greppable)  
- [ ] **Acceptance**: Given/When/Then; perf and security budgets  
- [ ] **Verification**: reproducible commands and expected exit codes  
- [ ] **Output**: diff vs wholeâ€‘file stated; JSON schema if parsed   
- [ ] **Split**: large scope broken into verifiable slices

---

## References (official OpenAI sources)

- **Introducing GPTâ€‘5 for developers** â€” model capabilities; `verbosity`, `reasoning_effort` (incl. `minimal`), custom tools.  
  https://openai.com/index/introducing-gpt-5-for-developers/  
- **GPTâ€‘5 Prompting Guide** â€” agentic eagerness control, tool preambles, markdown, planning.  
  https://cookbook.openai.com/examples/gpt-5/gpt-5_prompting_guide  
- **GPTâ€‘5: New Params & Tools** â€” verbosity, custom tools with freeâ€‘form payloads, minimal reasoning.  
  https://cookbook.openai.com/examples/gpt-5/gpt-5_new_params_and_tools  
- **Codex prompting guide** â€” pointers, verification steps, customizing agent behavior, splitting tasks.  
  https://developers.openai.com/codex/prompting  
- **Codex security guide** â€” sandbox/approvals, patchâ€‘based workflows, network policy.  
  https://developers.openai.com/codex/security  
- **Structured Outputs & Function Calling** â€” `strict: true` guarantees, JSON mode, tools.  
  https://help.openai.com/en/articles/8555517-function-calling-in-the-openai-api  
- **Reproducible outputs with `seed`** â€” cookbook example and cautions.  
  https://cookbook.openai.com/examples/reproducible_outputs_with_the_seed_parameter  
- **Prompt caching** â€” discount and placement of static vs dynamic content.  
  https://openai.com/index/api-prompt-caching/  
- **OpenAI Evals** â€” structured outputs/tools/webâ€‘search evals.  
  https://cookbook.openai.com/examples/evaluation/use-cases/structured-outputs-evaluation  
  https://cookbook.openai.com/examples/evaluation/use-cases/web-search-evaluation

---

*Prepared for experienced engineering teams integrating GPTâ€‘5 Codex with strong guardrails and a patchâ€‘based CI/CD workflow.*
